{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 動作確認用のファイル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tanka = pd.read_csv(\"kindai.csv\",header=None)\n",
    "waka = pd.read_csv(\"waka_half.csv\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13963 entries, 0 to 13962\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   0       13963 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 109.2+ KB\n",
      "None\n",
      "                                     0\n",
      "0  あああらき なげきやながき くろかみも ひとときにして あせやしぬべし\n",
      "1  あああらき なげきやなんじ れはかりは てんもうごけと なくにもにたる\n",
      "2  ああきみは いたましきまで ようちなる なさけにゑひぬ かなしからずや\n",
      "3  ああきみは かたるにたらず よわきじん よわきこいをば もてあそぶかな\n",
      "4  ああきみは すむむねもなき さびしさを かたるかなみだ かわきしめもて\n"
     ]
    }
   ],
   "source": [
    "print(tanka.info())\n",
    "print(tanka.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14292 entries, 0 to 14291\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   0       14292 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 111.8+ KB\n",
      "None\n",
      "                                     0\n",
      "0  あかからは みるへきものを かりかねの いつこはかりに なきてゆくらむ\n",
      "1  あかさりし おもかけはかり うつしても ちきりはさてや やまのゐのみつ\n",
      "2  あかさりし きみかにほひの こひしさに うめのはなをそ けさはをりつる\n",
      "3  あかさりし そてかとにほふ うめかかに おもひなくさむ あかつきのそら\n",
      "4  あかさりし そてのわかれの なみたより かたみかほなる つきそみにそふ\n"
     ]
    }
   ],
   "source": [
    "print(waka.info())\n",
    "print(waka.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_df(waka_df,tanka_df,n=3,text_num=1000):\n",
    "    \"\"\"\n",
    "    vector,idを保持するdfと、文ごとの単語の出現回数を保持するdfを取得する関数。\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        和歌or短歌のデータフレーム。\n",
    "    n : int\n",
    "        文を区切る文字数。\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    {waka or tanka}_count_id_df : pd.DataFrame\n",
    "        文ごとに出現したidの回数をカウントし、その数を保持するデータフレーム。\n",
    "    {waka or tanka}_id_list_df : pd.DataFrame\n",
    "        単語をidに置き換え、そのidの並びを保持するデータフレーム。\n",
    "    id_dict : dict\n",
    "        単語ごとにidを割り当て、その対応関係を保持する辞書。\n",
    "    \"\"\"\n",
    "    waka_list_df = waka_df.apply(split_text,axis=1)\n",
    "    tanka_list_df = tanka_df.apply(split_text,axis=1)\n",
    "    \n",
    "    waka_n_gram_df = waka_list_df.apply(n_gram,n=n)\n",
    "    tanka_n_gram_df = tanka_list_df.apply(n_gram,n=n)\n",
    "\n",
    "    id_dict = assign_id(pd.concat([waka_n_gram_df,tanka_n_gram_df],axis=0))\n",
    "\n",
    "    waka_id_list_df = convert_to_id(waka_n_gram_df,id_dict)\n",
    "    tanka_id_list_df = convert_to_id(tanka_n_gram_df,id_dict)\n",
    "\n",
    "    waka_count_id_df = count_id(waka_id_list_df,id_dict,start_point=0,text_num=text_num)\n",
    "    tanka_count_id_df = count_id(tanka_id_list_df,id_dict,start_point=0,text_num=text_num)\n",
    "\n",
    "    return waka_count_id_df,tanka_count_id_df,waka_id_list_df,tanka_id_list_df,id_dict\n",
    "\n",
    "def split_text(text_record):\n",
    "    return text_record.loc[0].split(\" \")\n",
    "\n",
    "def n_gram(text_list,n):\n",
    "    ans = []\n",
    "    for text in text_list:\n",
    "        gram = []\n",
    "        for i in range(len(text)-(n-1)):\n",
    "            gram.append(text[i:i+n])\n",
    "        ans.extend(gram)\n",
    "    return ans\n",
    "\n",
    "def assign_id(gram_df):\n",
    "    id_dict = {}\n",
    "    id_dict[\"id\"] = {}    # 単語ごとにidを割り当て\n",
    "    id_dict[\"word\"] = {}  # idからwordを取得する\n",
    "    id = 0\n",
    "    for line in gram_df:\n",
    "        for word in line:\n",
    "            if word not in id_dict[\"id\"].keys():\n",
    "                id_dict[\"id\"][word] = id\n",
    "                id_dict[\"word\"][id] = word\n",
    "                id += 1\n",
    "    return id_dict\n",
    "\n",
    "def count_word(df,dict):\n",
    "    dict[\"count\"] = {}\n",
    "    for line in df:\n",
    "        for word in line:\n",
    "            if word in dict[\"count\"].keys():\n",
    "                dict[\"count\"][word] += 1\n",
    "            else:\n",
    "                dict[\"count\"][word] = 1\n",
    "    return \n",
    "\n",
    "\n",
    "def convert_to_id(df,dict):\n",
    "    out_id_list = []\n",
    "    out_df = pd.DataFrame()\n",
    "    for i in range(df.shape[0]):\n",
    "        id_list = []\n",
    "        word_list = df.loc[i]\n",
    "        for word in word_list:\n",
    "            id_list.append(dict[\"id\"][word])\n",
    "        out_id_list.append(id_list)\n",
    "    out_df[\"id\"] = out_id_list\n",
    "    return out_df\n",
    "\n",
    "def count_id(df,dict,start_point=0,text_num=1000):\n",
    "    i = 0\n",
    "    count_id_df = pd.DataFrame(columns=dict[\"id\"])\n",
    "    height = count_id_df.shape[0]\n",
    "    width = count_id_df.shape[1]\n",
    "    end_point = start_point + text_num\n",
    "    for id_list in df[\"id\"][start_point:end_point]:\n",
    "        count_id_df.loc[i] = np.zeros(width,dtype=int)\n",
    "        for id in id_list:\n",
    "            count_id_df.loc[i,dict[\"word\"][id]] += 1\n",
    "        i += 1\n",
    "        if i==end_point:\n",
    "            return count_id_df\n",
    "    return count_id_df\n",
    "\n",
    "def save_df(n,waka_df,tanka_df,waka_save_dir,tanka_save_dir,waka_save_file,tanka_save_file,text_num=1000):\n",
    "    for i in range((waka.shape[0]//text_num)+1):\n",
    "        sp = i*text_num\n",
    "        waka_save_path = os.path.join(waka_save_dir,f\"{i}_\"+waka_save_file)\n",
    "        tanka_save_path = os.path.join(tanka_save_dir,f\"{i}_\"+tanka_save_file)\n",
    "        if sp==0:\n",
    "            (waka_count_id_df,tanka_count_id_df,waka_id_list_df,tanka_id_list_df,\n",
    "            vec_id_dict) = vectorize_df(waka_df,tanka_df,text_num=text_num)\n",
    "            waka_id_list_df.to_pickle(\"id_list_df/waka_id_df.pkl\")\n",
    "            tanka_id_list_df.to_pickle(\"id_list_df/tanka_id_df.pkl\")\n",
    "            with open(\"id_dict.pkl\",\"wb\") as f:\n",
    "                pickle.dump(vec_id_dict,f)\n",
    "        else:\n",
    "            waka_count_id_df = count_id(waka_id_list_df,vec_id_dict,start_point=sp,text_num=text_num)\n",
    "            tanka_count_id_df = count_id(tanka_id_list_df,vec_id_dict,start_point=sp,text_num=text_num)\n",
    "        waka_count_id_df.to_pickle(waka_save_path)\n",
    "        tanka_count_id_df.to_pickle(tanka_save_path)\n",
    "\n",
    "def concat_df(data_dir,save_dir,save_file):\n",
    "    save_name = os.path.join(save_dir,save_file)\n",
    "    df_list = []\n",
    "    for i,file_name in enumerate(os.listdir(data_dir)):\n",
    "        if file_name[0]==\".\":  # .DS_Storeを読み込まないようにするため\n",
    "            continue\n",
    "        with open(os.path.join(data_dir,file_name), mode=\"rb\") as f:\n",
    "            df_list.append(pickle.load(f))\n",
    "    save_df = pd.concat(df_list,axis=0).reset_index().drop(\"index\",axis=1)\n",
    "    save_df.to_pickle(save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "waka_count_id_path = f\"{n}_gram_waka_count_id.pkl\"\n",
    "tanka_count_id_path = f\"{n}_gram_tanka_count_id.pkl\"\n",
    "waka_cp_dir = \"waka_checkpoints\"\n",
    "tanka_cp_dir = \"tanka_checkpoints\"\n",
    "save_dir = \"save_count_id_df\"\n",
    "save_df(n,waka,tanka,waka_cp_dir,tanka_cp_dir,waka_count_id_path,tanka_count_id_path,text_num=10)\n",
    "\n",
    "concat_df(waka_cp_dir,save_dir,waka_count_id_path)\n",
    "concat_df(tanka_cp_dir,save_dir,tanka_count_id_path)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      あかか かから からは みるへ るへき へきも きもの ものを かりか りかね  ... もめじ ぶんさ へそは みるち へしり するゆ  \\\n",
      "0       0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   \n",
      "1       0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   \n",
      "2       0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   \n",
      "3       0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   \n",
      "4       0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   \n",
      "...    ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ...  ..  ..  ..  ..  ..  ..   \n",
      "14921   0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   \n",
      "14922   0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   \n",
      "14923   0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   \n",
      "14924   0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   \n",
      "14925   0   0   0   0   0   0   0   0   0   0  ...   0   0   0   0   0   0   \n",
      "\n",
      "      ゆめあ ばゆた てんぞ じるよ  \n",
      "0       0   0   0   0  \n",
      "1       0   0   0   0  \n",
      "2       0   0   0   0  \n",
      "3       0   0   0   0  \n",
      "4       0   0   0   0  \n",
      "...    ..  ..  ..  ..  \n",
      "14921   0   0   0   0  \n",
      "14922   0   0   0   0  \n",
      "14923   0   0   0   0  \n",
      "14924   0   0   0   0  \n",
      "14925   0   0   0   0  \n",
      "\n",
      "[14926 rows x 68050 columns]\n"
     ]
    }
   ],
   "source": [
    "with open(\"save_count_id_df/3_gram_tanka_count_id.pkl\", mode=\"rb\") as f:\n",
    "    df = pickle.load(f)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14926 entries, 0 to 14925\n",
      "Columns: 68050 entries, あかか to じるよ\n",
      "dtypes: object(68050)\n",
      "memory usage: 7.6+ GB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14292 entries, 0 to 14291\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      14292 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 111.8+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13963 entries, 0 to 13962\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      13963 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 109.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "with open(\"id_list_df/tanka_id_list_df.pkl\", mode=\"rb\") as f:\n",
    "    df_tanka_list = pickle.load(f)\n",
    "with open(\"id_list_df/waka_id_list_df.pkl\", mode=\"rb\") as f:\n",
    "    df_waka_list = pickle.load(f)\n",
    "print(df_waka_list.info())\n",
    "print(df_tanka_list.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[21, 22, 23, 24, 25, 26, 27, 14, 28, 29, 30, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[21, 22, 23, 41, 42, 43, 44, 45, 46, 47, 48, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[21, 22, 23, 59, 60, 61, 62, 63, 64, 65, 66, 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[21, 22, 23, 77, 78, 79, 80, 81, 82, 83, 84, 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  id\n",
       "0  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...\n",
       "1  [21, 22, 23, 24, 25, 26, 27, 14, 28, 29, 30, 3...\n",
       "2  [21, 22, 23, 41, 42, 43, 44, 45, 46, 47, 48, 4...\n",
       "3  [21, 22, 23, 59, 60, 61, 62, 63, 64, 65, 66, 6...\n",
       "4  [21, 22, 23, 77, 78, 79, 80, 81, 82, 83, 84, 8..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_waka_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14292\n",
      "13963\n"
     ]
    }
   ],
   "source": [
    "print(waka.shape[0])\n",
    "print(tanka.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = 0\n",
    "for w_list in df_waka_list.id:\n",
    "    if m1 < max(w_list):\n",
    "        m1 = max(w_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = 0\n",
    "for w_list in df_tanka_list.id:\n",
    "    if m2 < max(w_list):\n",
    "        m2 = max(w_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96060"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1+m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('kaggle_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "84bf7c439daa01440c423abe4b4b5ca63a847d1d90d0d19a84c063ae585be2da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
